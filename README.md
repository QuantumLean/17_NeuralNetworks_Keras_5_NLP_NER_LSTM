# Рекуррентные сети и модель LSTM в задачах обработки естественного языка
Учебно-методический материал для самостоятельного изучения 

Подготовлено: Артём Сучков

Recurrent newral networks (LSTM) ordered to NLP/NER tasks (RUS)
Educatorial for NER case studying \ LSTM neural network theory \ LSTM in NER-tasks example 
Edited by: Artem Suchkov

Пакет содержит: 
Лекционный материал в формате .ipnb
Датасеты для обучения (all, train, test). Внимание! Исходный набор данных не влез на GitHub. Ссылка на исходник см. в комментарии к 
Картинки к лекционному материалу.

Package includes: 
Lecture in script .ipnb
Cleaned and splited dataset (all, train, test). Цфктштп! Initial dataset heavy and not addded via GitHub. Links to initial see in comments to file ``...FINAL_all```.

#### Содержание:

##### Нйронная сеть Long Short Temp Memory
- Идея метода
- История создания
- Направления потоков и основные векторы ядра вычислений
- Алгоритмы ядра вычислений

##### Основы NLP. Введение. Токенизация в Python. 
- Введение в Natural Language Processing;
- Токенизация как метод преобразования данных;
- Основные сущности NLP, объекты токенизации;
- Основные задачи NLP в машинном обучении;
- Токенизация в решении NER задач машинного перевода.

Практика:
##### Модель №1: Решение задачи прогнозирования при помощи нейронной сети LSTM
- Разработка модели NMT
